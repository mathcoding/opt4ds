{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\"><b>Training Binary Neural Networks by Integer Linear Programming</b></span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://mate.unipv.it/gualandi\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Stefano Gualandi</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/mathcoding/opt4ds\" rel=\"dct:source\">https://github.com/mathcoding/opt4ds</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARK:** As usual, to install Gurobi on a Colab notebook, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if on Colab\n",
    "# %pip install gurobipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Binary) Neural Networks by Integer Linear Programming\n",
    "In this notebook, we show how to write an ILP model to train (binary) neural networks. We start by considering a basic perceptron, then we define multilayer neural networks, and, finally, we focus on binary neural networks. \n",
    "Notice that this type of approach will not scale with the size of the input, but it helps to reason about the optimal architecture of Neural Networks in small examples.\n",
    "\n",
    "Let the pair $\\mathcal{X} = (X, Y)$ be the input training dataset, where $X\\in \\mathbb{R}^{n \\times m}$ and $Y\\in \\mathbb{R}^n$.\n",
    "Each row $x_i \\in \\mathbb{R}^m$ of the matrix $X$ rapresent an input data point, which is mapped to the $i$-th label $y_i$.\n",
    "The dataset is partitioned into a training set $\\mathcal{T} \\subset \\mathcal{X}$ and a validat set $\\mathcal{S} \\subset \\mathcal{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic perceptron is defined by the following parametric function $F_w : \\mathbb{R}^{n \\times m} \\rightarrow \\mathbb{R}^m$ with parameter vector $w$:\n",
    "\n",
    "$$\n",
    "    \\hat{y}= F_w(X), \\text{ where } X \\in \\mathbb{R}^{n\\times m}, \\hat{y} \\in \\mathbb{R^n}, w \\in \\mathbb{R^n}\n",
    "$$\n",
    "\n",
    "The parametric function is defined as:\n",
    "\n",
    "$$\n",
    "    F_w(X) = \\text{sign}( X \\cdot w ) \n",
    "$$\n",
    "\n",
    "or componentwise:\n",
    "\n",
    "$$\n",
    "    f_w(x_i) = \\text{sign}( x_i^T \\cdot w ) \n",
    "$$\n",
    "\n",
    "where the sign() function is +1 for positive input, and -1 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the training dataset $\\mathcal{T}$, we are interested in finding the *best* parameters $w^*$:\n",
    "\n",
    "$$\n",
    "    w^* = \\argmin_{w \\in \\mathbb{R}^m} || y - \\hat{y} || = \\argmin_{w \\in \\mathbb{R}^m} || y - F_w(X) ||, \\quad \\text{ with } (X,y) \\in \\mathcal{T}\n",
    "$$\n",
    "\n",
    "Later, we evaluate the perceproton computing the accuracy on the validation set $\\mathcal{S}$:\n",
    "$$\n",
    "    \\text{accuracy} = \\frac{\\sum_{i=1}^n || y_i - f_{w^*}(x_i) ||}{n}, \\quad \\text{ with } (x_i,y_i) \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "We show next how to model using Integer Linear Programming the problem of finding the best $w^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning basic logical operators\n",
    "From a didatic perspective, learning the three basic logical operators **and**, **or**, and **xor** is an excellent exercise. Let us start with the **and** operator, whose true table is given below:\n",
    "\n",
    "| i | $x'_1(i)$ | $x'_2(i)$ | $y'(i)$ |\n",
    "|--|----|----|---|\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 2 | 0 | 1 | 0 |\n",
    "| 3 | 0 | 0 | 0 |\n",
    "| 4 | 1 | 1 | 1 |\n",
    "\n",
    "Moreover, we can reparametrize the data into the value -1,+1 (by using the transformation $x(i) = 2x'(i) - 1$ as follows.\n",
    "\n",
    "| i | $x_1(i)$ | $x_2(i)$ | $y(i)$ |\n",
    "|--|----|----|---|\n",
    "| 1 | -1 | -1 | -1 |\n",
    "| 2 | -1 | +1 | -1 |\n",
    "| 3 | -1 | -1 | -1 |\n",
    "| 4 | +1 | +1 | +1 |\n",
    "\n",
    "We can then use the values of $x_1$ and $x_2$ to define the training matrix $X\\in \\mathbb{R}^{4 \\times 2}$ and vector $y \\in \\mathbb{R}^4$.\n",
    "\n",
    "To start, we will use the true table as both the training and validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND function\n",
    "Xand = [(-1,-1), (-1, 1), (1,-1), (1,1)]\n",
    "Yand = [-1, -1, -1, 1]\n",
    "\n",
    "# OR function\n",
    "Xor = [(-1,-1), (-1, 1), (1,-1), (1,1)]\n",
    "Yor = [-1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, quicksum\n",
    "from random import randint, seed\n",
    "seed(13)\n",
    "\n",
    "def LogicalNN(Xs, Ys):\n",
    "    # Main ILP model\n",
    "    model = Model()\n",
    "\n",
    "    # TODO: complete the model\n",
    "    # ...\n",
    "\n",
    "    # Return weight of the final solution\n",
    "    return lambda x: 1 - 2*randint(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy input data\n",
    "To consider a more realistic setting, let us suppose that the input has a noise, that is, for instance:\n",
    "$$\n",
    "    y = f_w(X + \\epsilon)\n",
    "$$\n",
    "where $\\epsilon \\in \\mathbb{R}^{n \\times m}$ is any kind of noise coming from an unknown distribution (e.g., a uniform, normal, or lognormal).\n",
    "\n",
    "The following function can be used to add noise to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "def AddNoise(X, mu=0.1):\n",
    "    return list(map(lambda x: x+normal(0, mu), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase the size of the input dataset:\n",
    "T = 10\n",
    "Xs = T*[AddNoise(X, 0.1) for X in Xand]\n",
    "Ys = T*Yand\n",
    "print(Xs[:5], Ys[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1:** Adapt the previous script to be trained over random input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The **xor** logical function\n",
    "The simple perceptron is unable to correctly learn the **xor** function because it is not linearly separable. One possibility to overcome this limitation is to change the structure of the parametric function $f_W$.\n",
    "\n",
    "For instance, we could use the following function, by adding a new vector of parameters U:\n",
    "$$\n",
    "    \\hat{y} = f_{W,U} = \\text{sign}(U \\cdot \\text{sign}(X W)) \n",
    "$$\n",
    "\n",
    "Notice that now we have a *hidden* layer of unknows given by the inner $\\text{sign}$ function. The inner product between the weight $U$ and the result of the sign function, introduce bilinear terms that must be carfully linearized, in order to use Integer Linear Programming to solve the training problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR function\n",
    "Xxor = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "Yxor = [-1, 1, 1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(Xs, Ys, nh):\n",
    "    # Main ILP model\n",
    "    model = Model()\n",
    "\n",
    "    # TODO: complete the model\n",
    "    # ...\n",
    "\n",
    "    # Return weight of the final solution W, U\n",
    "    return lambda x: 1 - 2*randint(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you have to design the validation test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2:** Modify your script in such a way that the weight belong only to the set of values $\\{-1, 0, +1\\}$. Note that a value of 0 is equivalent to remove the corresponding link (and simplify the network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 3:** Can you *computationally proof* which is the smallest Binary Neural Network that can compute exactly the **xor** function? Hown many weights do you need in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of MNIST digits\n",
    "Starting from the solution used to model the **xor** function, you can build a parametric function whose parameters are fitted to classify images.\n",
    "\n",
    "You can use the same structure used for the **xor** function, but by chaning the number of states in the hidden layer:\n",
    "$$\n",
    "    \\hat{y} = f_{W,U} = \\text{sign}(U \\cdot \\text{sign}(X W)) \n",
    "$$\n",
    "\n",
    "Or you can propose any different type of solution.\n",
    "\n",
    "You can download the dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and exectue\n",
    "# !wget https://mate.unipv.it/gualandi/opt4ds/all_three_four.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and exectue\n",
    "# !wget https://mate.unipv.it/gualandi/opt4ds/all_nine_four.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the dataset\n",
    "To parse the dataset you can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Parse(filename):\n",
    "    fh = open(filename, 'r')\n",
    "\n",
    "    fh.readline()\n",
    "\n",
    "    Xs, Ys = [], []\n",
    "    for row in fh:\n",
    "        line = row.replace('\\n','').split(';')\n",
    "        Ys.append(int(line[0]))\n",
    "        Xs.append(list(map(int, line[1:])))\n",
    "\n",
    "    return np.matrix(Xs), np.array(Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys = Parse('../data/train_three_four.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of matrix X:', Xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of y:', Ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting digits\n",
    "To plot a digit you can use the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DrawDigit(A):\n",
    "    plt.imshow(A.reshape((28, 28)), cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawDigit(Xs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ys[0], Ys[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a NN\n",
    "To evaluate the accuracy of a Binary NN you can run the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccuracyMLP(Xs, Ys, F):\n",
    "    y_hat = np.array([F(x) for x in Xs])\n",
    "\n",
    "    n = len(Ys)\n",
    "    return (np.sum(Ys == y_hat))/n*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nh = number of activation in the hidden layer\n",
    "def TrainingMLP(Xs, Ys, nh=1):\n",
    "    # Main ILP model\n",
    "    model = Model()\n",
    "    # TO COMPLETE with your model\n",
    "\n",
    "    return lambda x: 1 - 2*randint(0,1)\n",
    "\n",
    "# Number of internal states\n",
    "nh = 2\n",
    "\n",
    "# REMARK: the predict function\n",
    "F = TrainingMLP(Xs, Ys, nh)\n",
    "\n",
    "# Evaluate accuracy (be careful of the matrix dimension)\n",
    "acc = AccuracyMLP(Xs, Ys, F)\n",
    "print('accuracy:', round(acc, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNN Classification Challenge\n",
    "For this challenge, you have to design a binary neural network, that is a binary neural network where all weights are either +1 or -1, and that is able to solve a binary classification problem defined on pair of MNIST images.\n",
    "\n",
    "For training, you will have two dataset, the first containing images of the digits 3 and 4, and the second, containing the images of digits 4 and 9. \n",
    "\n",
    "For the design phase, you should use a small number of input data points. Later, you can decide if having a *light* model that can take in input several data points, or a *heavy* model that can use only a few data points but is more general.\n",
    "\n",
    "The evaluation will be carried over a hidden mixed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*REMARK*: You have to submit your solution by June, Thursday 11, 2025, sending an email containing your python solution script.\n",
    "\n",
    "Partecipating to this (optional) challenge, you will get extra points for the final exam grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_grb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
