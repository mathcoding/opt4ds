{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\"><b>Training Binary Neural Networks by Integer Linear Programming</b></span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://mate.unipv.it/gualandi\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Stefano Gualandi</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/mathcoding/opt4ds\" rel=\"dct:source\">https://github.com/mathcoding/opt4ds</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Run the following script whenever running this script on a Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "if not shutil.which(\"pyomo\"):\n",
    "    !pip install -q pyomo\n",
    "    assert(shutil.which(\"pyomo\"))\n",
    "\n",
    "if not (shutil.which(\"glpk\") or os.path.isfile(\"glpk\")):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        !apt-get install -y -qq glpk-utils\n",
    "    else:\n",
    "        try:\n",
    "            !conda install -c conda-forge glpk \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARK:** If working on your personal computer, you might want to install [Gurobi with an academic license](https://www.gurobi.com/academia/academic-program-and-licenses/). Gurobi is a commercial solver that is extremely fast in solving large ILP instances, much more faster than the free GLPK solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training (Binary) Neural Networks by Integer Linear Programming\n",
    "In this notebook, we show how to write an ILP model to train (binary) neural networks. We start by considering a basic perceptron, then we defined multilayer neural network, and, finally, we focus on binary neural networks. \n",
    "Notice that this type of approach will not likely scale to the size of the input, but it can help to reason about the optimal architecture of Neural Networks in small examples.\n",
    "\n",
    "Let the pair $\\mathcal{X} = (X, Y)$ be the input training dataset, where $X\\in \\mathbb{R}^{n \\times m}$ and $Y\\in \\mathbb{R}^n$.\n",
    "Each row $x_i \\in \\mathbb{R}^m$ of the matrix $X$ rapresent an input data point, which is mapped to the $i$-th label $y_i$.\n",
    "The dataset is partinioned into a training set $\\mathcal{T} \\subset \\mathcal{X}$ and a validat set $\\mathcal{S} \\subset \\mathcal{T}$.\n",
    "\n",
    "A basic perceptron is defined by the following parametric function $f : \\mathbb{R}^{n \\times m} \\rightarrow \\mathbb{R}^n$:\n",
    "\n",
    "$$\n",
    "    \\hat{y}= f_W(X), \\text{ where } X \\in \\mathbb{R}^{n\\times m}, \\hat{y} \\in \\mathbb{R^n}, W \\in \\mathbb{R^n}\n",
    "$$\n",
    "\n",
    "The parametric function is defined as:\n",
    "\n",
    "$$\n",
    "    f_W(X) = \\text{sign}( X \\cdot W ) \n",
    "$$\n",
    "\n",
    "or componentwise:\n",
    "\n",
    "$$\n",
    "    f_W(x_i) = \\text{sign}( x_i \\cdot W ) \n",
    "$$\n",
    "\n",
    "where the sign function is +1 for positive input, and -1 otherwise.\n",
    "\n",
    "Using the training dataset $\\mathcal{T}$, we are interested in finding the *best* parameters $W^*$:\n",
    "\n",
    "$$\n",
    "    W^* = \\argmin_{W \\in \\mathbb{R}^n} || y - \\hat{y} || = \\argmin_{W \\in \\mathbb{R}^n} || y - f_W(X) ||, \\quad \\text{ with } (X,y) \\in \\mathcal{T}\n",
    "$$\n",
    "\n",
    "Later, we evaluate the perceproton computing the accuracy on the validation set $\\mathcal{S}$:\n",
    "$$\n",
    "    \\text{accuracy} = \\frac{\\sum || y - f_{W^*}(X) ||}{|\\mathcal{S}|}, \\quad \\text{ with } (X,y) \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "We show next how to model using Integer Linear Programming the problem of finding the best $W^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Learning basic logical operators\n",
    "From a didatic perspective, learning the three basic logical operators **and**, **or**, and **xor** is an excellent exercise. Let us start with the **and** operator, whose true table is given below:\n",
    "\n",
    "| i | $x'_1(i)$ | $x'_2(i)$ | $y'(i)$ |\n",
    "|--|----|----|---|\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 2 | 0 | 1 | 0 |\n",
    "| 3 | 0 | 0 | 0 |\n",
    "| 4 | 1 | 1 | 1 |\n",
    "\n",
    "Moreover, we can reparametrize the data into the value -1,+1 (by using the transformation $x(i) = 2x'(i) - 1$ as follows.\n",
    "\n",
    "| i | $x_1(i)$ | $x_2(i)$ | $y(i)$ |\n",
    "|--|----|----|---|\n",
    "| 1 | -1 | -1 | -1 |\n",
    "| 2 | -1 | +1 | -1 |\n",
    "| 3 | -1 | -1 | -1 |\n",
    "| 4 | +1 | +1 | +1 |\n",
    "\n",
    "We can then use the values of $x_1$ and $x_2$ to define the training matrix $X\\in \\mathbb{R}^{4 \\times 2}$ and vector $y \\in \\mathbb{R}^4$.\n",
    "\n",
    "To start, we will use the true table as both the training and validation dataset.\n",
    "\n",
    "TODO: complete with the notes from last lab session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND function (with first element for bias)\n",
    "Xand = [(1,-1,-1), (1,-1, 1), (1,1,-1), (1,1,1)]\n",
    "Yand = [-1, -1, -1, 1]\n",
    "\n",
    "# OR function (with first element for bias)\n",
    "Xor = [(1,-1,-1), (1,-1, 1), (1,1,-1), (1,1,1)]\n",
    "Yor = [-1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyomo.environ import ConcreteModel, Var, Objective, Constraint, SolverFactory\n",
    "from pyomo.environ import maximize, minimize, Binary, RangeSet, ConstraintList, NonNegativeReals, Reals, Integers\n",
    "\n",
    "def LogicalNN(Xs, Ys):\n",
    "    # Main Pyomo model\n",
    "    model = ConcreteModel()\n",
    "\n",
    "    # TODO: complete the model\n",
    "    # ...\n",
    "\n",
    "    # Return weight of the final solution\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a possible solution, look at the following link: \n",
    "\n",
    "https://github.com/mathcoding/opt4ds/blob/master/scripts/NN_and.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Noisy input data\n",
    "To consider a more realistic setting, let us suppose that the input has a noise, that is, for instance:\n",
    "$$\n",
    "    y = f_W(X + \\epsilon)\n",
    "$$\n",
    "where $\\epsilon \\in \\mathbb{R}^{n \\times m}$ is any kind of noise coming from an unknown distribution (e.g., a uniform, normal, or lognormal).\n",
    "\n",
    "The following function can be used to add noise to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "def AddNoise(X, mu=0.1):\n",
    "    return list(map(lambda x: x+normal(0, mu), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase the size of the input dataset:\n",
    "T = 10\n",
    "Xs = T*[AddNoise(X, 0.1) for X in Xand]\n",
    "Ys = T*Yand\n",
    "print(Xs[:5], Ys[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1:** Adapt the previous script to be trained over random input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 The **xor** logical function\n",
    "The simple perceptron is unable to correctly learn the **xor** function because it is not linearly separable. One possibility to overcome this limitation is to change the structure of the parametric function $f_W$.\n",
    "\n",
    "For instance, we could use the following function, by adding a new vector of parameters U:\n",
    "$$\n",
    "    \\hat{y} = f_{W,U} = \\text{sign}(U \\cdot \\text{sign}(X W)) \n",
    "$$\n",
    "\n",
    "Notice that now we have a *hidden* layer of unknows given by the inner $\\text{sign}$ function. The inner product between the weight $U$ and the result of the sign function, introduce bilinear terms that must be carfully linearized, in order to use Integer Linear Programming to solve the training problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: complete with the lab notes on the iPad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR function\n",
    "Xxor = [(1,-1,-1), (1,-1, 1), (1,1,-1), (1,1,1)]\n",
    "Yxor = [-1, 1, 1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(Xs, Ys, nh):\n",
    "    # Main Pyomo model\n",
    "    model = ConcreteModel()\n",
    "\n",
    "    # TODO: complete the model\n",
    "    # ...\n",
    "\n",
    "    # Return weight of the final solution W, U\n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you have to design the validation test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2:** Modify your script in such a way that the weight belong only to the set of values $\\{-1, 0, +1\\}$. Note that a value of 0 is equivalent to remove the corresponding link (and simplify the network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 3:** Can you proof which is the smallest Binary Neural Network that can compute exactly the **xor** function? Hown many weights do you need in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Classification of MNIST digits\n",
    "Starting from the solution used to model the **xor** function, you can build a parametric function whose parameters are fitted to classify images.\n",
    "\n",
    "You can use the same structure used for the **xor** function, but by chaning the number of states in the hidden layer:\n",
    "$$\n",
    "    \\hat{y} = f_{W,U} = \\text{sign}(U \\cdot \\text{sign}(X W)) \n",
    "$$\n",
    "\n",
    "Or you can propose any different type of solution.\n",
    "\n",
    "You can download the dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and exectue\n",
    "# !wget https://mate.unipv.it/gualandi/opt4ds/all_three_four.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and exectue\n",
    "# !wget https://mate.unipv.it/gualandi/opt4ds/all_none_four.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3.1 Parsing the dataset\n",
    "To parse the dataset you can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Parse(filename):\n",
    "    fh = open(filename, 'r')\n",
    "\n",
    "    fh.readline()\n",
    "\n",
    "    Xs, Ys = [], []\n",
    "    for row in fh:\n",
    "        line = row.replace('\\n','').split(';')\n",
    "        Ys.append(int(line[0]))\n",
    "        Xs.append(list(map(int, line[1:])))\n",
    "\n",
    "    return np.matrix(Xs), np.array(Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys = Parse('../data/train_three_four.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of matrix X:', Xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of y:', Ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3.2 Plotting digits\n",
    "To plot a digit you can use the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DrawDigit(A):\n",
    "    plt.imshow(A.reshape((28,28)), cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawDigit(Xs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.3 Evaluating a NN\n",
    "To evaluate the accuracy of a Binary NN you can run the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def AccuracyMLP(Xs, Ys, W, U):\n",
    "    y_hat = np.sign( np.matmul( np.sign(np.matmul(Xs, W)), np.transpose(U)))\n",
    "\n",
    "    n = len(Ys)\n",
    "    return (np.sum(Ys == y_hat))/n*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of internal states\n",
    "nh = 2\n",
    "\n",
    "# REMARK: the value of W and U should be the output of your neural network\n",
    "W = np.ones( (784, nh) )\n",
    "U = np.ones(nh)\n",
    "\n",
    "# Evaluate accuracy (be careful of the matrix dimension)\n",
    "acc = AccuracyMLP(Xs, Ys, W, U)\n",
    "print('accuracy:', round(acc, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 BNN Classification Challenge\n",
    "For this challenge, you have to design a binary neural network, that is a binary neural network where all weights are either +1 or -1, and that is able to solve a binary classification problem defined on pair of MNIST images.\n",
    "\n",
    "For training, you will have two dataset, the first containing images of the digits 3 and 4, and the second, containing the images of digits 4 and 9. \n",
    "\n",
    "For the design phase, you should use a small number of input data points. Later, you can decide if having a *light* model that can take in input several data points, or a *heavy* model that can use only a few data points but is more general.\n",
    "\n",
    "The evaluation will be carried over a hidden mixed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*REMARK*: You have to submit your solution by Thursday, 25th, 2023, sending an email containg your python solution script.\n",
    "\n",
    "Partecipating to this (optional) challenge, you will get extra points for the final exam grade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
