{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\"><b>Modeling Linear Regression by ILP</b></span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://mate.unipv.it/gualandi\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Stefano Gualandi</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>. Based on a project at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/mathcoding/opt4ds\" rel=\"dct:source\">https://github.com/mathcoding/opt4ds</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression by Integer Linear Programming\n",
    "In this notebook, we show how to find a LP model to perform linear regression. This is intended to be a modeling exercise from an optimization persepctive.\n",
    "\n",
    "The basic example of fitting a noisy $sin(x)$ function is freely inspired from Chapter 1 of [Deep Learning, Foundations and Concepts](https://link.springer.com/book/10.1007/978-3-031-45468-4) by [C.M. Bishop](https://scholar.google.co.uk/citations?user=gsr-K3ADUvAC&hl=en&oi=ao) and [H. Bishop](https://www.linkedin.com/in/hugh-bishop-119075154)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Introduction to linear regression\n",
    "The goal of linear regression is to predict a value $\\hat{y} \\in \\mathbb{R}$ corresponding to an input vector $\\hat{\\bm{x}} \\in \\mathbb{R}^m$ by using a linear **Machine Learning (LM)** model. The linear ML model can be any function $f_w : \\mathbb{R}^m \\rightarrow \\mathbb{R}$ that is linear in its parameter vector $w \\in \\mathbb{R}^{m+1}$. \n",
    "\n",
    "The **training** of the linear ML model consists in fitting the parametric function $f_w$ to a given set of $n$ samples points $(\\bm{x}_1, y_1), \\dots, (\\bm{x}_n, y_n)$. We start in this notebook by considering the problem of fitting the noisy function $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ equal to\n",
    "\n",
    "$$\n",
    "    y = f(x) = \\sin(x) + \\mathcal{N}(0, 0.1)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}(0, 0.1)$ is an additive Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to **train/fitting** a polynomial of degree $q$ to a given set of $n$ random samples on this noisy function. Your linear ML model is hence the following:\n",
    "\n",
    "$$\n",
    "    \\hat{y} = f_w(x) = \\sum_{i=0}^q w_i x^i = w_0 + w_1 x + \\dots + w_q x^q \n",
    "$$\n",
    "\n",
    "Note that this model is linear in the parameters $\\bm{w} \\in \\mathbb{R}^{q+1}$, but it is not linear in its input data $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a vector $\\bar{\\bm{w}} \\in \\mathbb{R}^{q+1}$, we can measure the error of the corresponding polynomial by computing the $p$ norm:\n",
    "\n",
    "$$\n",
    "    E(\\bar{\\bm{w}}) = || f_{\\bar{\\bm{w}}}(\\bm{x}) - \\bm{y} ||_p\n",
    "$$\n",
    "\n",
    "A common choice is to measure the error with a Root Mean Square (RMS) error:\n",
    "\n",
    "$$\n",
    "    E_{RMS}(\\bar{\\bm{w}}) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n | f_{\\bar{\\bm{w}}}(\\bm{x}_i) - \\bm{y}_i |^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since by fitting a model our main task is to **generalize** a function of the observed sample data, we are interested in the error on a set of $t$ **testing** sample points that are not used during fitting of the polynomial: the best ML model will obtain smaller errors on the testing sample points.\n",
    "\n",
    "For this reason, we distinguish between **training** errors and **testing** errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1:** You have to write an Integer Linear Programming (ILP) model that find the coefficient of the polynomial of order $q$ that generalize better the given sample of points. You can use any ILP model of your choice, and the objective is to get the lowest error on a testing set.\n",
    "\n",
    "**REMARK:** The model can be even a simple Linear Programming (LP) model, without integer variables.\n",
    "\n",
    "Below you will find some helper functions to generate the data, plotting the functions, and measuring the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generate and plotting the training data\n",
    "We start by writing a function generating $n$ random sample points in $[0, 2 \\pi]$, with a Gaussian noise with zero mean and stdev of 0.1.\n",
    "Note that we set the seed for reproducibility.\n",
    "\n",
    "Read the official documentation of the mean [numpy](https://numpy.org/) functions used: [uniform](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html), [normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html), and [np.sin](https://numpy.org/doc/stable/reference/generated/numpy.sin.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import pi\n",
    "from numpy.random import uniform, normal\n",
    "\n",
    "# Generate a set of sample points with white noise\n",
    "# N=Number of sample points\n",
    "def GenerateSample(n=25, stdev=0.1, seed=13):\n",
    "    np.random.seed(seed)\n",
    "    Xs = uniform(0, 2*pi, n)   \n",
    "    # Samples with white noise\n",
    "    Ys = np.sin(Xs) + normal(0.0, stdev, n)\n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the function with the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys = GenerateSample(n=200)\n",
    "print(Xs[:3])\n",
    "print(Ys[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the generating noisy function over the sample points with can use the following plotting function.\n",
    "\n",
    "Read the official documentation of the main functions: [linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html), [plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html), and [show](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def PlotSinSamples(Xs, Ys):\n",
    "    # Plot True sin function\n",
    "    D = np.linspace(0, 2*pi, 1000)\n",
    "    plt.plot(D, np.sin(D), color='blue', alpha=0.5)\n",
    "    # Plot sample points\n",
    "    plt.plot(Xs, Ys, 'o', color='red', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "PlotSinSamples(Xs, Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The fitted model is a python function\n",
    "Once you find the values for a vector $\\bm{w} \\in \\mathbb{R}^{q+1}$, your linear LM model (note that this is different from the LP model you use to find $\\bm{w}$), your parametric learned function can be coded with the following python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeFunction(Ws):\n",
    "    def f_w(x):\n",
    "        return sum(w*(x**j) for j, w in enumerate(Ws))\n",
    "    return f_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic line for the origin: y = x\n",
    "F_w = MakeFunction([0, -4, 1])\n",
    "print(F_w(4), F_w(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the predict values by your function $F$ over the domain $[0, 2\\pi]$, you can use the following plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PlotPolynomial(Xs, Ys, F):\n",
    "    # Plot True sin function\n",
    "    D = np.linspace(0, 2*pi, 1000)\n",
    "    plt.plot(D, np.sin(D), color='blue', alpha=0.5)\n",
    "    # Plot sample points\n",
    "    plt.plot(D, [F(x) for x in D], color='green', alpha=0.3)\n",
    "    # Plot sample points\n",
    "    plt.plot(Xs, Ys, 'o', color='red', alpha=0.5)\n",
    "    plt.plot(Xs, [F(x) for x in Xs], 'o', color='green', alpha=0.3)\n",
    "\n",
    "    plt.axis([0, 2*pi, -1.5, +1.5])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F_w = MakeFunction([-4, 0, 1])\n",
    "F_w = MakeFunction([-1, 0, 1, -0.5])\n",
    "\n",
    "PlotPolynomial(Xs, Ys, F_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Error evaluation\n",
    "Once you have trained your linear model (that is, you have found a vector $\\bm{w} \\in \\mathbb{R}^{q+1}$), you have to evaluate how your model has generalized over the training data.\n",
    "\n",
    "For this reason, first you have to sample a new set of testing point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "Xtest, Ytest = GenerateSample(n=50, seed=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you have to evaluate your model over the test set using the RMS error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMS(Xs, Ys, F):\n",
    "    return np.sqrt(sum((F(x) - y)**2 for x, y in zip(Xs, Ys)) / len(Xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current model, trainig RMS:', round(RMS(Xs, Ys, F_w), 4))\n",
    "print('Current model, testing RMS:', round(RMS(Xtest, Ytest, F_w), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Write an LP model to fit the polynomial coefficients\n",
    "Now, you have to write and solve an ILP model that find the best value for $\\bar{w}$, and return a prediction function, that given $\\bm{x}$ return a predicted target value $\\hat{y}$, using the previous `MakeFunction(Ws)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if on Colab\n",
    "# %pip install gurobipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, quicksum, Env\n",
    "\n",
    "def FittingByLP(Xs, Ys, q):\n",
    "    # LP model        \n",
    "    env = Env(params={'OutputFlag': 0})\n",
    "    model = Model(env=env)\n",
    "    \n",
    "    # Add variables\n",
    "    w = [model.addVar(lb=-GRB.INFINITY) for _ in range(q+1)]\n",
    "    z = [model.addVar(lb=0, obj=1.0) for _ in range(len(Xs))]\n",
    "\n",
    "    # Add constraints\n",
    "    for i in range(len(Xs)):\n",
    "        model.addConstr(z[i] >= quicksum((Xs[i]**j * w[j]) for j in range(q+1)) - Ys[i] )\n",
    "        model.addConstr(z[i] >= -quicksum((Xs[i]**j * w[j]) for j in range(q+1)) + Ys[i] )\n",
    "\n",
    "    model.optimize()\n",
    "    \n",
    "    # TODO: add solver status check\n",
    "    if model.Status != GRB.OPTIMAL:\n",
    "        print('LP solver failed')\n",
    "        return []\n",
    "    \n",
    "    # Return the prediction function\n",
    "    return MakeFunction([w[i].X for i in range(q+1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_LP = FittingByLP(Xs, Ys, 2)\n",
    "\n",
    "print('Current model, trainig RMS:', round(RMS(Xs, Ys, F_LP), 4))\n",
    "print('Current model, testing RMS:', round(RMS(Xtest, Ytest, F_LP), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_LP = FittingByLP(Xs, Ys, 12)\n",
    "PlotPolynomial(Xs, Ys, F_LP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have found your model, you should run experiments to study the trade off between the complexity of your model (i.e., tha maximum degree of the polynoial), the number of samples points (i.e., the size of your training dataset), and the different choices of your ILP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the complexity of the model\n",
    "def CheckComplexity(n=9, seed=13):\n",
    "    # Generate training data\n",
    "    Xs, Ys = GenerateSample(n, seed=seed)\n",
    "    Xt, Yt = GenerateSample(2*n, seed=seed)\n",
    "\n",
    "    for q in range(9):\n",
    "        F = FittingByLP(Xs, Ys, q)\n",
    "        if F is None:\n",
    "            print('ERROR: model was not fittted')\n",
    "        else:\n",
    "            print('q={}, Training size: {}, RMS: {}'.format(q, n, round(RMS(Xs, Ys, F), 4)))\n",
    "            print('q={}, Testing  size: {}, RMS: {}'.format(q, 2*n, round(RMS(Xt, Yt, F), 4)))\n",
    "            print()\n",
    "\n",
    "CheckComplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the complexity of the model\n",
    "def PlotComplexity(n=9, seed=13):\n",
    "    # Generate training data\n",
    "    Xs, Ys = GenerateSample(n, seed=seed)\n",
    "    Xt, Yt = GenerateSample(2*n, seed=seed)\n",
    "\n",
    "    TrainRMS = []\n",
    "    TestRMS = []\n",
    "    for q in range(9):\n",
    "        F = FittingByLP(Xs, Ys, q)\n",
    "        TrainRMS.append(RMS(Xs, Ys, F))\n",
    "        TestRMS.append(RMS(Xt, Yt, F))\n",
    "    \n",
    "    plt.plot(TrainRMS, label='Training', color='red')\n",
    "    plt.plot(TestRMS, label='Testing', color='blue')\n",
    "    plt.xlabel('Model complexity')\n",
    "    plt.ylabel('RMS')\n",
    "    plt.title('ML Model complexity vs RMS with {} sample points'.format(n))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "PlotComplexity(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 CHALLENGE: Predicting diabete\n",
    "In this challenge, you have to use a linear LM model to predict a target value that is a measure of disease (diabete) progression using 10 measurements. The [diabete dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html) is a standard dataset for linear regression and is available on the [sklearn](https://scikit-learn.org/) library.\n",
    "\n",
    "The following steps show how to import and set this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "rawdata = load_diabetes()\n",
    "\n",
    "print([k for k in rawdata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 target values\n",
    "rawdata.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 input vectors\n",
    "print(rawdata.data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set your data set\n",
    "Xs = rawdata.data\n",
    "Ys = rawdata.target\n",
    "\n",
    "# To split the train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, Ys, test_size=0.2, random_state=13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, check the documentation of the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) function.\n",
    "\n",
    "You the test data for training your model, and the test data for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ASSIGNMENT:** You have to implement the following function with the best ILP model to fit a linear regression model using Gurobi. The fit function must return a **predict** function, similarly to the function used to fit a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FittingDiabete(Xs, Ys):\n",
    "    # LP model        \n",
    "    env = Env(params={'OutputFlag': 0})\n",
    "    model = Model(env=env)\n",
    "    \n",
    "    # Add variables\n",
    "\n",
    "    # TODO: Complete with your LP model\n",
    "    # ....\n",
    "\n",
    "    \n",
    "    # Return the prediction function\n",
    "    def F(x):\n",
    "        # IMPLEMENT YOUR FITTING FUNCION, USING THE ILP SOLUTION\n",
    "        return 0\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in team of maximum 3 students, and return your solution (the fitting function) by email before April 23rd, 2025, at midnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 CHALLENGE: Predict insurance costs\n",
    "In this challenge, you have to use a linear LM model to predict the insurance costs (i.e., *charges*) by using a vector 6 elements.\n",
    "\n",
    "For the training you can use the *insurance_train.csv* file. For the evaluation, we will use an hidden test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and execute the following to get the data\n",
    "!wget https://raw.githubusercontent.com/mathcoding/opt4ds/master/data/insurance_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the CSV file are the following:\n",
    "\n",
    "$$age; sex; bmi; children; smoker; region; charges$$\n",
    "\n",
    "The last one is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('../data/insurance_train.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';')\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    for row in spamreader:\n",
    "        if row[0] != 'age':\n",
    "            line = list(map(lambda s: float(s.replace(',','.')), row))\n",
    "            Xs.append(line[:-1])\n",
    "            Ys.append(line[-1])  # last column is the target, that is the insurance price\n",
    "\n",
    "# If you prefer to work with numpy arrays\n",
    "import numpy as np\n",
    "Xs = np.matrix(Xs)\n",
    "Ys = np.array(Ys)\n",
    "print(Xs[:3])\n",
    "print(Ys[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ASSIGNMENT:** You have to implement a function with your best ILP model to fit a linear regression model using Gurobi. As the previous exercise, the fit function must return a **predict** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in team of maximum 3 students, and return your solution (the fitting function) by email before April 23rd, 2025, at midnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCORING**: Any team partecipating to the challenge will get an extra point at the exam. Any team winning one of the two challenge we will get a second extra point to the exam.\n",
    "\n",
    "**REMARK 1**: the extra point is valid only until September 2025.\n",
    "\n",
    "**REMARK 2**: using ChatGPT and/or GitHub Copilot is permitted, but it should be well-documented. Be carefull in using such tools!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_grb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
